--- 
knit: "bookdown::preview_chapter"
---

# (PART) Critical Appraisal {-}

# Searching the Literature {#literature}

The starting point of every research study is a literature review. To know where you are going, you need to know where the field has been. In some ways, technology makes this easier than it has been in the past, but we are swimming in information, and the pool gets deeper every day.

A lot deeper, actually.

In 2012, Google's former CEO Eric Schmidt [said that we create as much information every 2 days as we did from the beginning of time through 2003](http://techcrunch.com/2010/08/04/schmidt-data/). Two days! This is not just a bunch of cat photos either. [Global scientific output doubles every 9 years](http://blogs.nature.com/news/2014/05/global-scientific-output-doubles-every-nine-years.html). And with "[predatory journals](https://www.timeshighereducation.com/news/study-finds-eightfold-rise-predatory-journal-papers#survey-answer)" on the rise, this rate will only speed up. With so much noise around us, good information can be hard to find. 

This chapter presents a strategy for quickly getting a sense of the state-of-the-art of any health research topic and an outline for searching the literature for primary sources.

```{r cat, echo=FALSE}
knitr::include_url("http://giphy.com/embed/o0vwzuFwCGAFO",
                   height = "480px")
```

## Start with Systematic Reviews and Meta-Analyses

Repeat after me:

> I will not start my research by Googling "malaria." 
> I will not start my research by Googling "malaria." 
> I will not start my research by Googling "malaria."

If your research topic is malaria and you are not sure whether the vector is mosquitoes or monkeys, then [Wikipedia](https://en.wikipedia.org/wiki/Malaria) is probably a good place to start. There is no shame in that. Otherwise, begin by searching for relevant systematic reviews or meta-analyses. A good review is far superior to Googling "malaria" any day. 

### META-ANALYSIS

A **meta-analysis** is a quantitative approach to reviewing research in which the results from multiple studies are combined to estimate an overall effect size. The results of a meta-analysis are typically summarized in a forest plot like the one shown in Figure \@ref(fig:forestguide). Let's take a look at this helpful guide from @ried:2006 that breaks it all down.

```{r forestguide, fig.cap="Source: Ried (2006), http://bit.ly/2j9pfSz", echo=F}
knitr::include_graphics("images/forestguide.png", dpi = NA)
```

A forest plot summarizes the results of several studies that measured the effect of the same intervention on the same outcome. One study result is described and plotted per row, and the overall effect (i.e., the "pooled" or "meta" effect) of all the studies is displayed at the bottom. 

The study sample is divided into an intervention arm and a control arm, presented in the `n/N` format where `n` represents the number of participants who experienced a certain outcome and `N` is the total number of participants in the study arm. For example, 141 were people assigned to the intervention group in Study A. Of these 141 people, 1 person experienced the adverse outcome that the forest plot summarizes. 

Next, a plot of the **effect size** and the **confidence interval** is created. An effect size is a measure of the strength or magnitude of a relationship, such as the relationship between taking a medicine and experiencing a bad outcome. This guide shows a specific type of effect size: relative risk. Each study's point estimate of the relative risk is plotted around a line of "no effect." A risk of 1 means that there is no difference between the intervention and control groups. When the outcome is something bad, like death, the intervention should be designed to reduce the risk, which is represented by a risk ratio less than 1.

```{block, type='rmdtip'}
We talk about "estimates" of the effect because research can only approximate the truth. Every estimate has some uncertainty. In a forest plot, uncertainty is represented by confidence intervals.
```

The size of the effect estimate is based on how much the study contributed to the meta-analysis. All studies are not created equal, and the weight parameter lets researchers account for these differences in the analysis.

Each estimate point is surrounded by a confidence interval (typically 95%) that is summarized numerically in the final column. Basically, if a study is repeated 100 times, the effect size is expected to be within this interval 95% of the time. When this interval crosses the line of no effect, the effect could be null or could even run in the opposite direction. In this case, the result is not "statistically significant."

Finally, the test for **heterogeneity** is presented toward the bottom. Heterogeneity means diversity (and is the opposite of homogeneity). Heterogeneity in a forest plot refers to the diversity in effect size estimates across studies. Heterogeneity complicates the interpretation of a meta-analysis; it signals that we might be comparing apples and oranges. For instance, the intervention may work differently in different contexts, and the included studies were gathered from all over the world. In such a case, it might not make sense to attempt to determine one overall meta effect size from a comparison of the studies.

The first way to assess heterogeneity is to consider the plots. Do the confidence intervals from each study form a vertical column, even if the point estimates shift between them? If so, heterogeneity is probably low. Heterogeneity can also be summarized numerically. Two estimates of heterogeneity are often presented: chi-square (χ2) and I^2, which is generally preferred. Values greater than 75% may indicate that a change in the meta-analysis method (random vs fixed effects) is needed. If heterogeneity is reported with a high I^2 value, authors should address this in the methods or limitations section of the study.

```{block, type='rmdtip'}
@lewis:2001 discovered that the first forest plot was published in 1978, and first used in a meta-analysis in 1982. The name lagged behind, appearing first in 1996, apparently referring to the tree-line optics typical of most forest plots.
```

#### Example {-}

For example, consider the meta-analysis by Radeva-Petrova et al. [-@radevapetrova:2014]. The authors reviewed 17 studies of the effects of chemoprevention on pregnant women living in malaria-endemic areas. With their review, they set out to answer this basic question: 

> Do women who take antimalarial medication during pregnancy have a lower risk of getting infected with malaria, and thus a lower risk of experiencing the bad health outcomes associated with malaria?

One **indicator** of malaria infection is parasitemia, or the presence of malaria parasites in the blood. If chemoprevention has some preventive effect, less parasitemia should be observed among women exposed to the medication (i.e., treatment). Few interventions are 100% effective, so scientists often talk about reductions in the risk of bad outcomes like malaria.

The forest plot shown in Figure \@ref(fig:malariaforest) displays the results of 10 studies (8 trials) of cases of parasitemia among 3,663 pregnant women who were randomized to an **intervention group** (*n*=2,053) that received a preventive antimalarial drug or to a control group (*n*=1,610) that received a **placebo** (no drug). 

```{r malariaforest, fig.cap="Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj", echo=F}
knitr::include_graphics("images/rpforest.png", dpi = NA)
```

Details about each study are reported in separate rows in this figure. The study by Shulman et al. [-@shulman:1999] in row 6 found that 30 of the 567 women in the intervention group tested positive for parasitemia (i.e., malaria). Comparing this number to 199 of the 564 woman in the control group yields a risk ratio of 0.15, i.e., (30/567)/(199/564) = 0.15. In other words, the chemoprevention reduced the risk of parasitemia by 85%. This is a huge effect size!

The effect size for each study is presented in the far-right column and is depicted graphically in the size of the point estimate square. All point estimates fall to the left of the line of no effect (<1), thus indicating a favorable effect of the chemoprevention intervention, i.e., reduced risk of parasitemia. A risk ratio of 1 would indicate no difference in risk, and a ratio >1 would mean the risk was higher among the intervention group, thus favoring the control group (with no treatment). The overall (pooled) effect size is 0.39, or a 61% reduction in the risk of parasitemia.

Calculating this pooled effect size is not as simple as averaging the effects of the 10 studies because the studies were not given equal weight, as shown in the "weight" column. For instance, Greenwood et al. [-@greenwood:1989] had a sample size of only 34 children (i.e., 21+13=34). As a result, the effect size estimate is very noisy. The 95% confidence interval is very large and crosses 1. Consequently, the weight of this study is only 6.7%, which is lower than the others. Simply put, studies with weaker research designs, such as this one, have less weight in the pooled analysis.

A single forest plot provides a summary of the best available evidence and an estimate of the overall effect size, along with uncertainty intervals. A Google search cannot begin to offer that!

```{block, type='rmdshiny'}
Exploring meta-analysis
```

```{r shiny_ma, echo=F}
knitr::include_app("https://globalhealthresearch.shinyapps.io/meta-analysis-app/", height = "750px")
```

### SYSTEMATIC REVIEW

How did Radeva-Petrova et al. [-@radevapetrova:2014] find these studies in the first place? Through a **systematic review** of the literature. 

Most, if not all, meta-analyses are completed as part of a systematic review of the literature. Every systematic review is a type of literature review, but not every literature review is a systematic review, even if it is done *systematically.*

```{r reviewcircles, fig.cap="Literature reviews, systematic reviews, and meta-analyses", echo=F}
knitr::include_graphics("images/reviewcircles.png", dpi = NA)
```

As shown in the table below, a systematic review requires a number of steps that represent good practice, but they are too time-consuming for an initial general literature review. Nevertheless, the general process of creating a systematic review is important to evaluating the quality of the reviews gathered, and this process provides solid foundation for this part of the research process overall.

```{r sysrev, echo=F}
sysrev <- data.frame(sr=c(
"The goal of a systematic review is to be comprehensive and to include every relevant article.",
"For this reason, most systematic reviews are conducted by teams, given the large scope of the data initially collected for most research topics.",
"Like any other aspect of research, however, systematic reviews must define and follow a method that can be replicated.",
"Most systematic reviews preregister the research plan, meaning that the authors submit their planned methods to a registry like [PROSPERO](http://www.crd.york.ac.uk/PROSPERO/) prior to conducting the study. Preregistration gives other researchers confidence that the team is not selectively choosing advantageous results at the end to make an interesting paper. This registration informs other researchers that a group is working on a certain area of study, which can discourage duplicate research efforts that may, therefore, fail to be published.",
"These preregistration plans include a specific search strategy using specific search terms for individual scholarly databases so other researchers can recreate the search.",
"Importantly, both inclusion criteria and exclusion criteria must be clearly outlined when a systematic review is undertaken. One inclusion criteria might be that assignment to study arms had to be random; an exclusion criteria might be all studies without a control arm that used a placebo. Most systematic searches specify several, if not many, criteria regarding which studies to include or exclude. Team members screen the search results and sort them according to these criteria, beginning with titles and abstract reviews and moving to full-text reviews later.",
"In systematic reviews, specific details are extracted from every study included, such as numbers of participants, methods, analysis techniques, and key outcomes.",
"In addition, the research team formally assesses the quality of each study, including the potential for bias, and these assessments are considered when the results are synthesized."),
lr=c("Literature reviews, on the other hand, do not follow such rigid or explicit methods. They are not expected to be exhaustive.",
"Literature reviews can usually be conducted by a single person rather than a team",
"Literature reviews, on the other hand, don't have to follow such rigid methods or make the methods explicit.",
"Not the case for literature reviews.",
"It's a good idea to do the same for a literature review, even if not a strict requirement.",
"Screening for a literature review is typically less intensive.",
"An annotated bibliography might suffice for a literature review.",
"This process is more ad hoc for literature reviews."))
names(sysrev) <- c("Systematic Reviews", "Literature Reviews")

knitr::kable(sysrev, format = "html", 
caption = 'Comparing systematic reviews and literature reviews.'
) %>%
  html_table_width(c(300,300))
```
 
#### Where to find systematic reviews{-}

Three excellent sources for finding systematic reviews (and meta-analyses) in global health are the [Cochrane Library](http://www.cochranelibrary.com/), the [Campbell Collaboration](http://www.campbellcollaboration.org/), and [3ie](http://www.3ieimpact.org/evidence/systematic-reviews/). Many of the reviews in these databases can be accessed by searching within PubMed using the [Clinical Queries](http://www.ncbi.nlm.nih.gov/pubmed/clinical/) feature.

#### How to read systematic reviews{-}

##### Abstract and plain language summary{-}

Cochrane reviews follow a standard format that can look overwhelming at first, but this format actually makes them quite easy to read and understand. As with most journal articles, Cochrane reviews begin with an *Abstract* and a *Plain language summary,* which can be helpful for newcomers to a the topic. For example, Radeva-Petrova et al. [-@radevapetrova:2014] include the following passage in their plain language summary:

> For women in their first or second pregnancy, malaria chemoprevention prevents moderate to severe anemia (high quality evidence); and prevents malaria parasites being detected in the blood (high quality evidence). It may also prevent malaria illness. We don't know if it prevents maternal deaths, as this would require very large studies to detect an effect.

This paragraph brings us up to speed with the state of the science for preventing malaria and its effects among pregnant women living in malaria-endemic areas (and points to some gaps in the literature). Google does not filter the evidence in this manner, and starting with systematic reviews pays off almost every time.

##### Summary tables{-}

Next come the *Summary tables*, such as the one presented below from Radeva-Petrova et al. [-@radevapetrova:2014]. These tables provide enough information to make an initial judgment about the study.  

```{r summary1, fig.cap="Malaria chemoprevention for pregnant women living in endemic areas. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj", echo=F}
knitr::include_graphics("images/summary1.png")
```

First, the comparative risk column shows the assumed risk among the control group. For instance, the risk of antenatal parasitemia is 286 events per every 1,000 people. This is the median control group risk across 8 trials of 3,663 women. The relative risk is 0.39. Recall that this is the pooled, or "meta," effect size. The corresponding risk among the intervention group is `286*0.39=111` per 1,000 people.[^tbldetails] 

As shown in the final column, the quality of this evidence is rated "high." Here, the authors are referring to [GRADE criteria](http://www.gradeworkinggroup.org), a systematic approach to evaluating the quality of empirical evidence:

1. **High**—Further research is very unlikely to change our confidence in the estimate of effect.
2. **Moderate**—Further research is likely to have an important impact on our confidence in the estimate of effect and may change the estimate.
3. **Low**—Further research is very likely to have an important impact on our confidence in the estimate of effect and is likely to change the estimate.
4. **Very Low**—We are very uncertain about the estimate.

[^tbldetails]: For more information on summary tables, see [here](http://www.cochranelibrary.com/about/explanations-for-cochrane-summary-of-findings-sof-tables.html).

##### Background{-}

The abstract contains much important information, such as summary text and tables, and sometimes even forest plots. The abstract is often followed by a *Background* section, typically a short overview that explains which knowledge gaps the review is intended to fill. Radeva-Petrova et al. [-@radevapetrova:2014] have used this section to present a conceptual framework for malaria prevention during pregnancy.

```{r concept, fig.cap="Drugs for preventing malaria in pregnancy: conceptual framework. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj", echo=F}
knitr::include_graphics("images/concept.png")
```

##### Methods{-}

The *Methods* section details how the review was organized and conducted. The purpose of this section is to provide enough detail to enable other researchers to replicate the review. These are the main components:[^prisma]

1. A description of the population and intervention
2. The key outcomes of interest
3. The search strategy and databases
4. Inclusion and exclusion criteria for studies reviewed
5. Procedures for extracting information from each study
6. Procedures for assessing bias and conducting a meta-analysis (if one is included)

[^prisma]: See the [PRISMA Statement](http://www.equator-network.org/reporting-guidelines/prisma/) for a checklist of components to include in each section.

##### Results{-}

The *Results* section typically begins with details about how many primary articles were identified, screened, and excluded. This information is typically presented graphically in a flow diagram like the one below from Radeva-Petrova et al. [-@radevapetrova:2014].

```{r flowdiag, fig.cap="Study flow diagram. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj", echo=F, fig.height=5}
knitr::include_graphics("images/flowdiag.png")
```

Once the included studies are identified, the review authors usually report on the quality of the evidence presented in each study. The nature of these sources of bias are discussed in a later chapter, but the heatmaps are introduced here. Although they may appear complex at first, they provide a useful summary of bias. Basically, the green circles indicate that the authors believe that the research is not affected by bias; conversely, red indicates the likely presence of bias, in the authors’ judgment. The less bias, i.e., the more green circles, the higher the quality of the studies included in the review.

```{r bias, fig.cap="Risk of bias summary: review authors’ judgements about each risk of bias item for each included trial. Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj", echo=F}
knitr::include_graphics("images/bias.png")
```

##### Discussion and conclusions{-}

The discussion section provides a short summary of the findings, commentary on the quality of the evidence, and thoughts about what the review adds to the existing literature on the topic. The discussion tends to be short relative to the size of the overall review. 

The discussion section is often followed by a brief statement of the authors' conclusions. Here, the authors frame the overall results in terms of their implications for practice and research. 

Radeva-Petrova et al. [-@radevapetrova:2014] conclude:

> Routine chemoprevention to prevent malaria and its consequences has been extensively tested in RCTs, with clinically important benefits on anemia and parasitaemia in the mother, and on birth-weight in infants.

Simply put, the final conclusion of this review is "chemoprevention works."

##### Appendices{-}

Often, appendices include table after table of data included and (sometimes) excluded studies. They are often followed by dozens of forest plots if the systematic review includes a meta-analysis with several outcomes or populations of interest. Reading the appendices provides a sense of the mechanics behind a systematic review. Radeva-Petrova et al. [-@radevapetrova:2014] wrap up on page 120!

## Devising a Search Strategy

Not every topic has been the subject of a recent systematic review or meta-analysis. In such cases, a search of the primary literature is needed. The first step to a successful search is establishing a clear definition of the objective.

### ASKING A RESEARCH QUESTION

Again, the helpful mnemonic [PICO](http://guides.mclibrary.duke.edu/c.php?g=158178&p=1035882), introduced in the previous chapter, can guide the formation of a good clinical question.

```{r pico, echo=FALSE}
  pico <- data.frame(letter=c("**P**", "**I**", "**C**", "**O**"),
                     label=c("Patient, Population, or Problem",
                             "Intervention, Prognostic Factor, or Exposure",
                             "Comparison",
                             "Outcome"))
  names(pico) <- NULL
  knitr::kable(pico, col.names=NA)
```

As an example, PICO can be used to develop a well-focused, searchable research question on treating malaria during pregnancy. The **problem** is malaria infections. The **population** is pregnant women living in malaria-endemic areas.

Not every clinical question involves testing of a treatment or **intervention**, but we'll focus a lot on these types of questions in this book. For the example at hand, the intervention is malaria chemoprevention. [Prognostic factor refers to covariates that could influence the prognosis of the patient. An exposure would be something that we think might increase the risk of an outcome.]

Similarly, not every question involves a **comparison** group.[^RCTcomp] In this example, the comparison is no intervention or a placebo in place of the drug being administered.

[^RCTcomp]: Randomized trials always do.

There are many potential **outcomes** for treating malaria; in this case, the outcome is parasitemia.

Combining all of this information yields a research question like this one:

> Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitemia?

```{block, type='rmdtip'}
Remember, not all research questions fit neatly into the PICO format. [Chapter 2](#research101) included another mnemonic helper: FINER.
```

### APPROACHES

With your basic research question outlined, you're ready to begin searching. In the beginning you might take a Quick and Dirty™ approach to get started. Eventually you'll need to graduate to a proper search strategy to be more systematic, even if the end goal is not a formal systematic review.  

#### Quick and Dirty™{-}

A reasonable initial approach is to find a few recent articles to get a quick sense of what is out there. Possibly, Google Scholar *could* be helpful here. For instance, an advanced Google Scholar search for PICO terms "malaria pregnant chemoprevention parasitemia" (limited to recent years) identified a paper by Braun et al. [-@braun:2015] on the use of intermittent preventive treatment in pregnancy (IPTp) with sulfadoxine–pyrimethamine (SP)—a specific type of chemoprevention—on malaria infections among pregnant women in western Uganda.

```{block, type='rmdtip'}
Customize your Google Scholar experience by clicking on the gear icon. Enable use of a bibliography manager, and click on "Library links" to add your library to get links to full text.
```

An article's **keywords** provide a good starting point for any search. Not all journals print keywords, however, but if it does, they are usually listed directly following the abstract. 

Following the abstract and the keywords is the **Introduction**. Some journals, especially in the fields of medicine and public health, have very brief introduction sections that might not be of much help. 

The **Discussion** section, which usually directly follows the Results section, may also hold new search leads. Authors typically use the discussion to link the study results to the existing literature to demonstrate how the results add to what is already known.

After the introduction and discussion sections, the **references** provide a plethora of useful information. First, the names of journals that publish studies in the field are always listed with their article titles. . If a certain journal appears commonly, a scan of the journal's table of contents for recent issues could be useful.[^hand]

[^hand]: This is referred to as "hand searching".

```{block, type='rmdtip'}
[*Journal Citation Reports*](https://en.wikipedia.org/wiki/Journal_Citation_Reports) is a useful resource about the scholarly journals in a field. This annual report ranks the journals in each field according to impact factors. [Impact factors](https://en.wikipedia.org/wiki/Impact_factor) are one metric used to evaluate the importance of a journal in its field. 
```

#### More systematic{-}

##### Planning and documenting the search{-}

In both formal and informal systematic reviews, it is important to plan and document the search. Literature reviews do not need to be as thorough as systematic reviews, but the approach can be nicely overlaid. Consider the work of Radeva-Petrova et al. [-@radevapetrova:2014] for some inspiration.

Every good systematic review includes a table or appendix like this one to make the method reproducible. In other words, running this search query at the same time on two different computers should yield the same results.

```{r search, fig.cap="Source: Radeva-Petrova et al. (2014), http://bit.ly/1U3q2Oj", echo=F}
knitr::include_graphics("images/search.png")
```

For the purposes of a literature review, the results do not need to be directly repeatable by others, but you should be certain that you recreate your search if needed.

```{block, type='rmdtip'}
Most databases require that users create an account within the database to log in and save the search approaches. This makes it easier to retrace the search steps later. Also, differences in the design of each database and interface often require distinct ways of customizing a search. To conduct an actual systematic review for publication—as opposed to just searching the literature systematically—research librarians are the “go to” resources for building search strategies.
```

##### Selecting a database{-}

As you can see from the table, Radeva-Petrova et al. [-@radevapetrova:2014] searched five databases. MEDLINE is probably the most well known of this group. [PubMed](http://www.ncbi.nlm.nih.gov/pubmed) includes the MEDLINE database. PubMed is a very good place to start to find health-related studies. Research librarians are also an excellent resource to determine whether other databases are suitable for the topic under research.

##### Generating search terms{-}

Once the proper database is identified, specific search terms are needed. These usually coincide with the keywords published in related articles.

[**MeSH**](http://www.ncbi.nlm.nih.gov/mesh) is another resource for identifying articles to include in a literature review or a systematic review. MeSH, which stands for "Medical Subject Headings," is a controlled vocabulary thesaurus that is used to index articles in PubMed. This thesaurus is helpful because there are often many ways to refer to the same phenomenon. For instance, the MeSH term for "breast cancer" is "breast neoplasm." When a search for "breast cancer" is conducted in PubMed, the database helps by casting a wider net using the MeSH term automatically. Notably, however, PubMEd does not always do this! What PubMed includes in the search can be viewed by clicking on the search details link after running a search):

`"breast neoplasms"[MeSH Terms] OR ("breast"[All Fields] AND "neoplasms"[All Fields]) OR "breast neoplasms"[All Fields] OR ("breast"[All Fields] AND "cancer"[All Fields]) OR "breast cancer"[All Fields]`

Not surprisingly, breast cancer is referred to using many, many terms! The following entry terms are indexed to the MeSH term "breast neoplasms" by humans at PubMed:

* Breast Neoplasm
* Neoplasm, Breast
* Neoplasms, Breast
* Tumors, Breast
* Breast Tumors
* Breast Tumor
* Tumor, Breast
* Mammary Neoplasms, Human
* Human Mammary Neoplasm
* Human Mammary Neoplasms
* Neoplasm, Human Mammary
* Neoplasms, Human Mammary
* Mammary Neoplasm, Human
* Mammary Carcinoma, Human
* Carcinoma, Human Mammary
* Carcinomas, Human Mammary
* Human Mammary Carcinomas
* Mammary Carcinomas, Human
* Human Mammary Carcinoma
* Breast Cancer
* Cancer, Breast
* Cancer of Breast
* Mammary Cancer
* Malignant Neoplasm of Breast
* Malignant Tumor of Breast
* Breast Carcinoma
* Cancer of the Breast

Back in the world of mosquitoes, the MeSH term for "malaria" is "malaria," conveniently, and a search for this term in PubMed actually searches:

`"malaria"[MeSH Terms] OR malaria[All fields]`

The following entry terms are indexed to the MeSH term "malaria":

* Remittent Fever
* Fever, Remittent
* Paludism
* Plasmodium Infections
* Infections, Plasmodium
* Infection, Plasmodium
* Plasmodium Infection
* Marsh Fever
* Fever, Marsh

##### Running a search{-}

Once the initial search terms have been identified, a query can be built. Query construction is an iterative process, full of trial and error. 

```{r boo, fig.cap="Boolean operators: AND, OR, NOT", echo=F}
knitr::include_graphics("images/boolean.jpg", dpi = NA)
```

Some basic Boolean operators are needed to conduct effective searches: AND, OR, NOT. For instance, consider the search PubMed runs when the terms "malaria OR pregnancy" are entered:

`("malaria"[MeSH Terms] OR "malaria"[All Fields]) OR ("pregnancy"[MeSH Terms] OR "pregnancy"[All Fields])`

These four terms are combined with `OR`, meaning we keep results that match *any* of these terms. At the time of this writing, PubMed returns 922,588 results. 

Of course, it would make more sense to search for "malaria AND pregnancy," instead of "malaria OR pregnancy":

`("malaria"[MeSH Terms] OR "malaria"[All Fields]) AND ("pregnancy"[MeSH Terms] OR "pregnancy"[All Fields])`

The first two terms and last two terms are combined separately with `OR`. These combinations are then combined with `AND` (notice the use of parentheses to segment the operations), dropping the pool of results to 4,203 records. The `AND` operator will always maintain or decrease the number of results.  

To limit the results humans, we can add `AND "humans"[MeSH Terms]` to the end.[^humans] Doing so drops the pool of results to 3,798. 

[^humans]: PubMed allows the researcher to limit results to humans or animals from the results page with one click, so it is not essential to use Boolean operators manually—PubMed does it automatically.

`("malaria"[MeSH Terms] OR "malaria"[All Fields]) AND ("pregnancy"[MeSH Terms] OR "pregnancy"[All Fields]) AND "humans"[MeSH Terms]`

Alternatively, the `NOT` operator could be used to limit the results to nonhumans, but `NOT` is not commonly used. 

Combining the components of the PICO questions and Boolean operators can be very useful. Consider the following research question: 

> Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia?

Here's what we could do in plain English:

* pregnancy `OR` pregnant women
* `AND` malaria endemic
* `AND` chemoprevention (to search for specific drugs, string them together with `OR`s)
* `AND` randomized controlled trial
* `AND` parasitaemia

At this writing, this search returned 513 records in PubMed. 

After the results have been honed adequately, the same search can be applied in another database, especially if the topic crosses disciplinary boundaries, like economics and health. Research librarians can provide invaluable leads to important databases.  

Oh hey, here's one now:

```{block, type='rmdplay'}
```

```{r fivetips, echo=F}
knitr::include_url("https://www.youtube.com/embed/6wWeeCBBlk4")
```

##### Screening results{-}

Even the best search queries return some duds, so all search results must be screened. Radeva-Petrova et al. [-@radevapetrova:2014] have used a very thorough approach. Most literature reviews are not as detailed.

Often systematic review searches return hundreds or thousands of potential hits, so a study team screens titles and abstracts to exclude obvious outliers. As this process begins, team members commonly screen some of the same records and discuss any differences in their findings to establish **reliability** (i.e., consistency). Basically, everyone screening records should make the same inclusion/exclusion decision.

The Radeva-Petrova et al. [-@radevapetrova:2014] search strategy turned up 179 unique records, and the authors excluded 126 of these records after screening the abstracts. The excluded studies did not meet certain *predefined* criteria. For instance, the authors only wanted to include studies using RCTs and quasi-experimental designs

Fifty-three studies remained for full-text review. Only 17 of these 53 studies still met eligibility criteria after this step.[^foot22]

[^foot22]: These 17 trials were described in 22 articles. 

##### Supplemental searches{-}

It is customary in a systematic review–and helpful in general reviews—to augment database searches with reference reviews and hand searches to ensure that no key references were missed in the database query. A reference review is nothing more than a scan of an eligible article's bibliography. In a hand search, eligible articles are plucked from the tables of contents on the website of the journal for each issue published during the search window. If either supplemental method turns up a lot of new results, the systematic review search strategy should be revised to be more comprehensive.

##### Extracting data{-}

Depending on the research objectives data can be systematically extracted from a study or set of studies, such as key facts related to study design, methods, and results. If it fulfills the objective, however, an annotated bibliography can be conducted in a much shorter time at a smaller scale. 

Where a true systematic review is used, a data extraction strategy is also needed. The PICO research question can help guide the identification of the minimum data to extract. Returning to the example of Radeva-Petrova et al. [-@radevapetrova:2014]:  

> Among pregnant women living in malaria-endemic areas, is chemoprevention more effective than a placebo at preventing parasitaemia?

Some meaningful data items to extract include:

* Study setting/population
* Sample size
* Sample demographics, including parity
* Study design
* Intervention details, such as specific medication and dose
* Primary outcome (e.g., parasitaemia)
* Effect size

Numerous software options exist for storing extracted data, but a simple spreadsheet with rows of studies and columns of study variables is often sufficient. Many teams use this approach for large systematic reviews, and it works well for more modest research and less experienced researchers, too.

## Use a Reference Manager

The importance of using a software program for managing references cannot be overstated. The manual collation and assembly of a bibliography is simply a colossal waste of time.

Several reference managers are available. [Zotero](https://www.zotero.org/) is free and open source. Some other options are not really free. With EndNote, for example, a university may make offer a free download for enrolled students, but the license expires upon graduation or soon becomes obsolete without a paid upgrade. Additionally, in global health, some colleagues may not have access to a program like EndNote, which makes collaboration challenging. For these reasons, Zotero is good choice.

Although a tutorial is beyond the scope of this chapter, some features that are common to many reference managers are helpful:

* Easily imports references from databases like PubMed; moves from the search results to the reference manager instantly
* Automatically retrieves full-text PDFs
* Syncs PDFs to tablets and phones
* Connects to word processing software; inserting references in papers is easy
* Automatically creates bibliographies based on works cited
* Instantly reformats in-text citations and references to different styles, such as APA, AMA, or Harvard
* Shares collections by automatically sync-ing via the Cloud to facilitate collaboration.
* Easily exports references to other reference managers

## Why Does Any of This Matter?

Most of the time, literature searches are not conducted to perform a research-related systematic review. Mostly, literature searches offer the most expedient means of staying current with developments in a specific field and of illuminating gaps in the collective knowledge. Literature searches lead to FINER research questions: questions that are **I**nteresting, and **N**ovel, and **R**elevant. Literature also provides insight into how researchers of a specific topic or in a particular field conceptualize study designs, plan study measurement, and report results. 

Searches, quick data evaluations, and rapid integration of study summaries take practice. For researchers early in the process, using effective search strategies can save a great deal of time and energy.

## Share Feedback{-}

This book is a work in progress. You'd be doing me a big favor by taking a moment to tell me what you think about this chapter.

```{r CH03bfeedback, echo=F}
knitr::include_url("https://duke.qualtrics.com/SE/?SID=SV_737a1xu4Igskgxn",
height="600px")
```
